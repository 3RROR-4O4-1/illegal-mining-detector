{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e8a484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from PIL import Image\n",
    "import time\n",
    "import planetary_computer as pc\n",
    "from pystac_client import Client\n",
    "import rasterio\n",
    "from rasterio.warp import transform_bounds\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "import cv2\n",
    "import gc\n",
    "\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2045ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Configuration parameters for the illegal mining detection pipeline.\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# DATA PATHS\n",
    "# =============================================================================\n",
    "\n",
    "# Local mine images (JPG files from Landsat) step 1\n",
    "LOCAL_MINE_DIR = \"GreenAI/src/data/landsat_converted/2019/barragem_jpg\"\n",
    "\n",
    "# CSV with mine coordinates (lon, lat columns) step 1\n",
    "MINE_COORDINATES_CSV = \"GreenAI/src/data/zones_centroids.csv\"\n",
    "\n",
    "# Output directory for all pipeline outputs\n",
    "OUTPUT_DIR = \"GreenAI/src/output\"\n",
    "\n",
    "# =============================================================================\n",
    "# DATA COLLECTION SETTINGS\n",
    "# =============================================================================\n",
    "\n",
    "# Brazil bounding box (lon_min, lon_max, lat_min, lat_max)\n",
    "BRAZIL_BOUNDS = (-75, -35, -35, 5)\n",
    "# Number of forest (negative) samples to generate\n",
    "N_FOREST_SAMPLES = 800  # amount of random samples to draw\n",
    "# Satellite imagery settings (Landsat)\n",
    "DATE_RANGE = \"2023-01-01/2025-10-31\"\n",
    "MAX_CLOUD_COVER = 20\n",
    "# Image size - distance from center point in km when retrieving data\n",
    "IMAGE_SIZE_KM = 2.5\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET BUILDING SETTINGS\n",
    "# =============================================================================\n",
    "\n",
    "# Match fetched images to local Landsat style\n",
    "MATCH_TO_LANDSAT_STYLE = True #was created to evaluate if style transfer helps\n",
    "# Augmentation settings\n",
    "AUGMENTATION_STRENGTH = \"medium\"  # how many augmentations per image\n",
    "N_AUGMENTED_PER_IMAGE = 5\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL TRAINING SETTINGS\n",
    "# =============================================================================\n",
    "\n",
    "# Model architecture\n",
    "BACKBONE = \"resnet34\"  # \"resnet18\", \"resnet34\", \"efficientnet_b0\"\n",
    "HAS_SEGFORMER = True\n",
    "HAS_TORCH = True\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16  # how many at once\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 300\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "VALIDATION_SPLIT = 0.15\n",
    "\n",
    "# Image size for model input  224 resnet \n",
    "MODEL_IMAGE_SIZE = 224\n",
    "\n",
    "# =============================================================================\n",
    "# INFERENCE SETTINGS\n",
    "# =============================================================================\n",
    "\n",
    "# Confidence threshold for mining detection\n",
    "MINING_THRESHOLD = 0.5\n",
    "\n",
    "# =============================================================================\n",
    "# VALIDATION SETTINGS\n",
    "# =============================================================================\n",
    "\n",
    "# Path to CSV with known mining coordinates for validation\n",
    "# Expected columns: lat, lon, label (where label is \"mining\" or \"forest\")\n",
    "VALIDATION_CSV = \"GreenAI/src/data/known_mining_sites.csv\"\n",
    "\n",
    "# Default radius for overview (10km from center = 20km x 20km area)\n",
    "OVERVIEW_RADIUS_KM = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#satellite fetcher class\n",
    "\"\"\"\n",
    "Satellite image fetching from Microsoft Planetary Computer.\n",
    "Uses Sentinel-2 but simulates Landsat-upscaled aesthetics to match local training data.\n",
    "\n",
    "Optimized for memory efficiency with large images.\n",
    "\"\"\"\n",
    "\n",
    "PROTECTED_AREAS = [\n",
    "    {\"name\": \"Tumucumaque\", \"lat\": 1.5, \"lon\": -52.5, \"radius_km\": 100},\n",
    "    {\"name\": \"Jaú\", \"lat\": -2.0, \"lon\": -63.0, \"radius_km\": 80},\n",
    "    {\"name\": \"Mamirauá\", \"lat\": -2.5, \"lon\": -65.0, \"radius_km\": 50},\n",
    "    {\"name\": \"Terra do Meio\", \"lat\": -5.5, \"lon\": -53.0, \"radius_km\": 100},\n",
    "    {\"name\": \"Xingu\", \"lat\": -10.5, \"lon\": -52.5, \"radius_km\": 100},\n",
    "]\n",
    "\n",
    "\n",
    "def center_to_bbox(\n",
    "    center_lat: float,\n",
    "    center_lon: float,\n",
    "    radius_km: float\n",
    ") -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Convert center coordinate + radius to bounding box.\n",
    "    \n",
    "    Args:\n",
    "        center_lat: Latitude of center point\n",
    "        center_lon: Longitude of center point  \n",
    "        radius_km: Distance from center to edge in km\n",
    "        \n",
    "    Returns:\n",
    "        (lon_min, lat_min, lon_max, lat_max)\n",
    "    \"\"\"\n",
    "    # Latitude: 1 degree ≈ 111 km\n",
    "    lat_offset = radius_km / 111.0\n",
    "    \n",
    "    # Longitude: depends on latitude (narrower near poles)\n",
    "    lon_offset = radius_km / (111.0 * np.cos(np.radians(center_lat)))\n",
    "    \n",
    "    return (\n",
    "        center_lon - lon_offset,  # lon_min\n",
    "        center_lat - lat_offset,  # lat_min\n",
    "        center_lon + lon_offset,  # lon_max\n",
    "        center_lat + lat_offset   # lat_max\n",
    "    )\n",
    "\n",
    "\n",
    "class SatelliteFetcher:\n",
    "    def __init__(\n",
    "        self,\n",
    "        date_range: str = \"2023-01-01/2024-12-31\",\n",
    "        max_cloud_cover: int = 20\n",
    "    ):\n",
    "        self.collection = \"sentinel-2-l2a\"\n",
    "        self.date_range = date_range\n",
    "        self.max_cloud_cover = max_cloud_cover\n",
    "\n",
    "        self.client = Client.open(\n",
    "            \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "            modifier=pc.sign_inplace\n",
    "        )\n",
    "        self.bands = [\"B04\", \"B03\", \"B02\"]\n",
    "    \n",
    "    def fetch_image(\n",
    "        self,\n",
    "        lat: float,\n",
    "        lon: float,\n",
    "        distance_km: float = 2.5,\n",
    "        target_stats: Optional[Dict] = None,\n",
    "        output_size: int = 512,\n",
    "        simulate_landsat: bool = True\n",
    "    ) -> Tuple[Optional[np.ndarray], Dict]:\n",
    "        \"\"\"\n",
    "        Fetch a satellite image centered on a point.\n",
    "        \n",
    "        Args:\n",
    "            lat: Latitude of center point\n",
    "            lon: Longitude of center point\n",
    "            distance_km: Distance from center to edge in km\n",
    "            target_stats: Color statistics to match (optional)\n",
    "            output_size: Output image size in pixels\n",
    "            simulate_landsat: If True, applies blur to match Landsat-upscaled quality\n",
    "            \n",
    "        Returns:\n",
    "            (RGB image as numpy array, metadata dict)\n",
    "        \"\"\"\n",
    "        bbox = center_to_bbox(lat, lon, distance_km)\n",
    "        \n",
    "        try:\n",
    "            search = self.client.search(\n",
    "                collections=[self.collection],\n",
    "                bbox=bbox,\n",
    "                datetime=self.date_range,\n",
    "                query={\"eo:cloud_cover\": {\"lt\": self.max_cloud_cover}}\n",
    "            )\n",
    "            \n",
    "            items = list(search.items())\n",
    "            if not items:\n",
    "                return None, {\"error\": \"No imagery found\", \"lat\": lat, \"lon\": lon}\n",
    "            \n",
    "            items.sort(key=lambda x: x.properties.get(\"eo:cloud_cover\", 100))\n",
    "            item = items[0]\n",
    "            \n",
    "            rgb_bands = []\n",
    "            for band_name in self.bands:\n",
    "                href = item.assets[band_name].href\n",
    "                with rasterio.open(href) as src:\n",
    "                    src_bbox = transform_bounds('EPSG:4326', src.crs, *bbox)\n",
    "                    window = src.window(*src_bbox)\n",
    "                    data = src.read(1, window=window)\n",
    "                    if data.size == 0:\n",
    "                        return None, {\"error\": \"Empty\", \"lat\": lat, \"lon\": lon}\n",
    "                    rgb_bands.append(data.astype(np.float32))\n",
    "            \n",
    "            # Align shapes\n",
    "            min_h = min(b.shape[0] for b in rgb_bands)\n",
    "            min_w = min(b.shape[1] for b in rgb_bands)\n",
    "            rgb_bands = [b[:min_h, :min_w] for b in rgb_bands]\n",
    "\n",
    "            rgb = np.stack(rgb_bands, axis=-1)\n",
    "            del rgb_bands  # Free memory\n",
    "            \n",
    "            # Normalization (in-place where possible)\n",
    "            np.clip(rgb, 0, 10000, out=rgb)\n",
    "            rgb /= 10000.0\n",
    "            rgb_enhanced = self._enhance_contrast(rgb)\n",
    "            del rgb\n",
    "            \n",
    "            if target_stats:\n",
    "                rgb_final = self._normalize_to_target(rgb_enhanced, target_stats)\n",
    "                del rgb_enhanced\n",
    "            else:\n",
    "                rgb_final = rgb_enhanced\n",
    "            \n",
    "            rgb_uint8 = (np.clip(rgb_final, 0, 1) * 255).astype(np.uint8)\n",
    "            del rgb_final\n",
    "            \n",
    "            img = Image.fromarray(rgb_uint8)\n",
    "            del rgb_uint8\n",
    "            \n",
    "            if simulate_landsat:\n",
    "                low_res_size = 93 \n",
    "                img_small = img.resize((low_res_size, low_res_size), Image.BILINEAR)\n",
    "                img = img_small.resize((output_size, output_size), Image.BICUBIC)\n",
    "                platform_note = \"Sentinel-2 (Downsampled to match Landsat)\"\n",
    "            else:\n",
    "                img = img.resize((output_size, output_size), Image.LANCZOS)\n",
    "                platform_note = \"Sentinel-2 (Full Resolution)\"\n",
    "            \n",
    "            metadata = {\n",
    "                \"lat\": lat, \"lon\": lon,\n",
    "                \"platform\": platform_note,\n",
    "                \"cloud_cover\": item.properties.get(\"eo:cloud_cover\", None),\n",
    "                \"datetime\": item.properties.get(\"datetime\", None)\n",
    "            }\n",
    "            \n",
    "            return np.array(img), metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, {\"error\": str(e), \"lat\": lat, \"lon\": lon}\n",
    "    \n",
    "    def fetch_overview(\n",
    "        self,\n",
    "        center_lat: float,\n",
    "        center_lon: float,\n",
    "        radius_km: float = 10.0,\n",
    "        max_dimension: int = 2048,\n",
    "        target_stats: Optional[Dict] = None\n",
    "    ) -> Tuple[Optional[np.ndarray], Dict]:\n",
    "        \"\"\"\n",
    "        Fetch a HIGH-RESOLUTION overview image centered on a point.\n",
    "        \n",
    "        This method fetches Sentinel-2 imagery at full resolution (no blur simulation)\n",
    "        for use with SegFormer segmentation.\n",
    "        \n",
    "        Args:\n",
    "            center_lat: Latitude of center point\n",
    "            center_lon: Longitude of center point\n",
    "            radius_km: Distance from center to edge in km (default 10km = 20km x 20km area)\n",
    "            max_dimension: Maximum width or height of output image\n",
    "            target_stats: Color statistics to match (optional)\n",
    "            \n",
    "        Returns:\n",
    "            (RGB image as numpy array, metadata dict with bounds info)\n",
    "        \"\"\"\n",
    "        bbox = center_to_bbox(center_lat, center_lon, radius_km)\n",
    "        lon_min, lat_min, lon_max, lat_max = bbox\n",
    "        \n",
    "        lat_span_km = (lat_max - lat_min) * 111.0\n",
    "        lon_span_km = (lon_max - lon_min) * 111.0 * np.cos(np.radians(center_lat))\n",
    "        \n",
    "        print(f\"    Fetching overview: {lon_span_km:.1f} km x {lat_span_km:.1f} km\")\n",
    "        print(f\"    Center: ({center_lat:.4f}, {center_lon:.4f}), Radius: {radius_km} km\")\n",
    "        \n",
    "        try:\n",
    "            search = self.client.search(\n",
    "                collections=[self.collection],\n",
    "                bbox=list(bbox),\n",
    "                datetime=self.date_range,\n",
    "                query={\"eo:cloud_cover\": {\"lt\": self.max_cloud_cover}}\n",
    "            )\n",
    "            \n",
    "            items = list(search.items())\n",
    "            if not items:\n",
    "                return None, {\"error\": \"No imagery found\", \"bbox\": bbox}\n",
    "            \n",
    "            items.sort(key=lambda x: x.properties.get(\"eo:cloud_cover\", 100))\n",
    "            item = items[0]\n",
    "            \n",
    "            print(f\"    Found {len(items)} images, using best with {item.properties.get('eo:cloud_cover', '?')}% cloud\")\n",
    "            \n",
    "            # Load all bands and determine actual shapes from data\n",
    "            rgb_bands = []\n",
    "            native_shape = None\n",
    "            \n",
    "            for band_name in self.bands:\n",
    "                href = item.assets[band_name].href\n",
    "                with rasterio.open(href) as src:\n",
    "                    src_bbox = transform_bounds('EPSG:4326', src.crs, *bbox)\n",
    "                    window = src.window(*src_bbox)\n",
    "                    data = src.read(1, window=window)\n",
    "                    \n",
    "                    if data.size == 0:\n",
    "                        return None, {\"error\": \"Empty window\", \"bbox\": bbox}\n",
    "                    \n",
    "                    if native_shape is None:\n",
    "                        native_shape = data.shape\n",
    "                        print(f\"    Native resolution: {data.shape[1]}x{data.shape[0]} pixels\")\n",
    "                    \n",
    "                    rgb_bands.append(data.astype(np.float32))\n",
    "                    del data\n",
    "                \n",
    "                gc.collect()\n",
    "            \n",
    "            # Use minimum dimensions across bands (they can differ slightly)\n",
    "            min_h = min(b.shape[0] for b in rgb_bands)\n",
    "            min_w = min(b.shape[1] for b in rgb_bands)\n",
    "            \n",
    "            # Stack into single array, trimming to common dimensions\n",
    "            rgb = np.zeros((min_h, min_w, 3), dtype=np.float32)\n",
    "            for i, band in enumerate(rgb_bands):\n",
    "                rgb[:, :, i] = band[:min_h, :min_w]\n",
    "            \n",
    "            del rgb_bands\n",
    "            gc.collect()\n",
    "            \n",
    "            # In-place normalization to save memory\n",
    "            np.clip(rgb, 0, 10000, out=rgb)\n",
    "            rgb /= 10000.0\n",
    "            \n",
    "            # Enhance contrast (returns new array, but we delete old immediately)\n",
    "            rgb_enhanced = self._enhance_contrast(rgb)\n",
    "            del rgb\n",
    "            gc.collect()\n",
    "            \n",
    "            if target_stats:\n",
    "                rgb_final = self._normalize_to_target(rgb_enhanced, target_stats)\n",
    "                del rgb_enhanced\n",
    "            else:\n",
    "                rgb_final = rgb_enhanced\n",
    "            \n",
    "            # Convert to uint8 (final output format)\n",
    "            rgb_uint8 = (np.clip(rgb_final, 0, 1) * 255).astype(np.uint8)\n",
    "            del rgb_final\n",
    "            gc.collect()\n",
    "            \n",
    "            # Resize if needed\n",
    "            h, w = rgb_uint8.shape[:2]\n",
    "            if max(h, w) > max_dimension:\n",
    "                scale = max_dimension / max(h, w)\n",
    "                new_w = int(w * scale)\n",
    "                new_h = int(h * scale)\n",
    "                img = Image.fromarray(rgb_uint8)\n",
    "                del rgb_uint8\n",
    "                img = img.resize((new_w, new_h), Image.LANCZOS)\n",
    "                rgb_uint8 = np.array(img)\n",
    "                del img\n",
    "                print(f\"    Resized to: {new_w}x{new_h} pixels\")\n",
    "            \n",
    "            final_h, final_w = rgb_uint8.shape[:2]\n",
    "            meters_per_pixel_x = (lon_span_km * 1000) / final_w\n",
    "            meters_per_pixel_y = (lat_span_km * 1000) / final_h\n",
    "            \n",
    "            metadata = {\n",
    "                \"bbox\": bbox,\n",
    "                \"center_lat\": center_lat,\n",
    "                \"center_lon\": center_lon,\n",
    "                \"radius_km\": radius_km,\n",
    "                \"width_km\": lon_span_km,\n",
    "                \"height_km\": lat_span_km,\n",
    "                \"image_width\": final_w,\n",
    "                \"image_height\": final_h,\n",
    "                \"meters_per_pixel\": (meters_per_pixel_x + meters_per_pixel_y) / 2,\n",
    "                \"platform\": \"Sentinel-2 (Full Resolution)\",\n",
    "                \"cloud_cover\": item.properties.get(\"eo:cloud_cover\", None),\n",
    "                \"datetime\": item.properties.get(\"datetime\", None),\n",
    "                \"native_resolution\": native_shape\n",
    "            }\n",
    "            \n",
    "            return rgb_uint8, metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None, {\"error\": str(e), \"bbox\": bbox}\n",
    "    \n",
    "    def _enhance_contrast(self, rgb: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Enhance contrast using percentile stretching.\"\"\"\n",
    "        result = np.zeros_like(rgb, dtype=np.float32)\n",
    "        for i in range(3):\n",
    "            band = rgb[:, :, i]\n",
    "            valid = band[band > 0.001]\n",
    "            if len(valid) > 0:\n",
    "                p2, p98 = np.percentile(valid, [2, 98])\n",
    "                if p98 > p2:\n",
    "                    result[:, :, i] = (band - p2) / (p98 - p2)\n",
    "                else:\n",
    "                    result[:, :, i] = band\n",
    "            else:\n",
    "                result[:, :, i] = band\n",
    "        return np.clip(result, 0, 1)\n",
    "    \n",
    "    def _normalize_to_target(self, rgb: np.ndarray, target_stats: Dict) -> np.ndarray:\n",
    "        \"\"\"Normalize colors to match target statistics.\"\"\"\n",
    "        result = np.zeros_like(rgb, dtype=np.float32)\n",
    "        for i, channel in enumerate([\"r\", \"g\", \"b\"]):\n",
    "            band = rgb[:, :, i]\n",
    "            valid = band[band > 0.001]\n",
    "            if len(valid) > 0:\n",
    "                src_p2, src_p98 = np.percentile(valid, [2, 98])\n",
    "                tgt_p2 = target_stats.get(f\"{channel}_p2\", 0.0)\n",
    "                tgt_p98 = target_stats.get(f\"{channel}_p98\", 1.0)\n",
    "                if src_p98 > src_p2:\n",
    "                    normalized = (band - src_p2) / (src_p98 - src_p2)\n",
    "                    result[:, :, i] = normalized * (tgt_p98 - tgt_p2) + tgt_p2\n",
    "                else:\n",
    "                    result[:, :, i] = band\n",
    "            else:\n",
    "                result[:, :, i] = band\n",
    "        return np.clip(result, 0, 1)\n",
    "    \n",
    "    def generate_forest_samples(\n",
    "        self,\n",
    "        n_samples: int,\n",
    "        output_dir: str,\n",
    "        target_stats: Optional[Dict] = None,\n",
    "        brazil_bounds: Tuple[float, float, float, float] = (-75, -35, -35, 5),\n",
    "        distance_km: float = 2.5\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Generate random forest samples from protected areas.\"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        samples = []\n",
    "        attempts = 0\n",
    "        max_attempts = n_samples * 10 \n",
    "        \n",
    "        print(f\"    Targeting {n_samples} samples (simulating upscaled quality)...\")\n",
    "        \n",
    "        while len(samples) < n_samples and attempts < max_attempts:\n",
    "            attempts += 1\n",
    "            area = random.choice(PROTECTED_AREAS)\n",
    "            offset_km = random.uniform(0, area[\"radius_km\"])\n",
    "            angle = random.uniform(0, 2 * np.pi)\n",
    "            lat = area[\"lat\"] + (offset_km / 111) * np.sin(angle)\n",
    "            lon = area[\"lon\"] + (offset_km / 111) * np.cos(angle)\n",
    "            \n",
    "            rgb, metadata = self.fetch_image(lat, lon, distance_km=distance_km, target_stats=target_stats)\n",
    "            \n",
    "            if rgb is not None:\n",
    "                mean_val = rgb.mean()\n",
    "                if 20 < mean_val < 230:\n",
    "                    filename = f\"forest_{len(samples):04d}.jpg\"\n",
    "                    filepath = output_path / filename\n",
    "                    Image.fromarray(rgb).save(filepath, quality=95)\n",
    "                    metadata[\"filename\"] = filename\n",
    "                    samples.append(metadata)\n",
    "                    if len(samples) % 10 == 0:\n",
    "                        print(f\"    [+] Generated {len(samples)}/{n_samples} forest samples\")\n",
    "                \n",
    "                del rgb\n",
    "                gc.collect()\n",
    "        \n",
    "        with open(output_path / \"metadata.json\", \"w\") as f:\n",
    "            json.dump(samples, f, indent=2)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312142d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_statistics(image_dir: str, sample_pixels: int = 100000) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract RGB statistics from a directory of images.\n",
    "    Uses reservoir sampling for memory efficiency.\n",
    "    \n",
    "    Args:\n",
    "        image_dir: Directory containing JPG/PNG images\n",
    "        sample_pixels: Max pixels to sample (default 100k uses ~1.2MB)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with per-channel statistics\n",
    "    \"\"\"\n",
    "    image_path = Path(image_dir)\n",
    "    image_files = list(image_path.glob(\"*.jpg\")) + list(image_path.glob(\"*.png\"))\n",
    "    \n",
    "    if not image_files:\n",
    "        raise ValueError(f\"No images found in {image_dir}\")\n",
    "    \n",
    "    # Pre-allocate fixed-size reservoir (memory-efficient)\n",
    "    reservoir = np.zeros((sample_pixels, 3), dtype=np.float32)\n",
    "    total_seen = 0\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        img = np.array(Image.open(img_file).convert(\"RGB\"), dtype=np.float32) / 255.0\n",
    "        pixels = img.reshape(-1, 3)  # Flatten to (N, 3)\n",
    "        \n",
    "        for px in pixels:\n",
    "            if total_seen < sample_pixels:\n",
    "                reservoir[total_seen] = px\n",
    "            else:\n",
    "                # Reservoir sampling: replace with probability sample_pixels/total_seen\n",
    "                j = random.randint(0, total_seen)\n",
    "                if j < sample_pixels:\n",
    "                    reservoir[j] = px\n",
    "            total_seen += 1\n",
    "        \n",
    "        del img, pixels  # Free memory\n",
    "    \n",
    "    # Use only filled portion\n",
    "    n = min(total_seen, sample_pixels)\n",
    "    samples = reservoir[:n]\n",
    "    \n",
    "    stats = {\n",
    "        \"r_mean\": float(np.mean(samples[:, 0])),\n",
    "        \"r_std\": float(np.std(samples[:, 0])),\n",
    "        \"r_p2\": float(np.percentile(samples[:, 0], 2)),\n",
    "        \"r_p98\": float(np.percentile(samples[:, 0], 98)),\n",
    "        \n",
    "        \"g_mean\": float(np.mean(samples[:, 1])),\n",
    "        \"g_std\": float(np.std(samples[:, 1])),\n",
    "        \"g_p2\": float(np.percentile(samples[:, 1], 2)),\n",
    "        \"g_p98\": float(np.percentile(samples[:, 1], 98)),\n",
    "        \n",
    "        \"b_mean\": float(np.mean(samples[:, 2])),\n",
    "        \"b_std\": float(np.std(samples[:, 2])),\n",
    "        \"b_p2\": float(np.percentile(samples[:, 2], 2)),\n",
    "        \"b_p98\": float(np.percentile(samples[:, 2], 98)),\n",
    "        \n",
    "        \"n_images\": len(image_files),\n",
    "        \"n_pixels_sampled\": n,\n",
    "        \"source_dir\": str(image_dir)\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbfc6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 1: Collect Mine Images (Positive Samples)\n",
    "# =============================================================================\n",
    "\n",
    "def step1_collect_mines() -> Dict:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 1: Collecting Mine Images (Positive Samples)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    output_path = Path(OUTPUT_DIR) / \"raw_data\" / \"mines\"\n",
    "    if output_path.exists():\n",
    "        shutil.rmtree(output_path)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    local_path = Path(LOCAL_MINE_DIR).resolve()\n",
    "    csv_path = Path(MINE_COORDINATES_CSV).resolve()\n",
    "    \n",
    "    print(f\"  Local mine directory: {local_path}\")\n",
    "    \n",
    "    local_images = []\n",
    "    local_image_size = None\n",
    "    \n",
    "    if local_path.exists():\n",
    "        print(\"  Scanning for *_512.jpg files...\")\n",
    "        local_images = list(local_path.glob(\"*_512.jpg\"))\n",
    "        \n",
    "        if not local_images:\n",
    "            print(\"  ⚠ No *_512.jpg files found. Falling back to all *.jpg files.\")\n",
    "            local_images = list(local_path.glob(\"*.jpg\")) + list(local_path.glob(\"*.png\"))\n",
    "        else:\n",
    "            print(f\"  ✓ Found {len(local_images)} specific 512px images.\")\n",
    "\n",
    "        if local_images:\n",
    "            \n",
    "            first_img = Image.open(local_images[0])\n",
    "            local_image_size = first_img.size\n",
    "            print(f\"  Local image size: {local_image_size[0]}x{local_image_size[1]} pixels\")\n",
    "    \n",
    "    if local_images:\n",
    "        print(f\"  Copying {len(local_images)} local images to output...\")\n",
    "        for i, img_file in enumerate(local_images):\n",
    "            dst_name = f\"mine_local_{i:04d}.jpg\"\n",
    "            shutil.copy(img_file, output_path / dst_name)\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"    Copied {i + 1}/{len(local_images)}\")\n",
    "    else:\n",
    "        print(\"  ⚠ No local images found!\")\n",
    "\n",
    "    landsat_stats = None\n",
    "    if local_images:\n",
    "        print(\"  Extracting color statistics from selected images...\")\n",
    "        landsat_stats = extract_statistics(str(output_path))\n",
    "        \n",
    "        stats_file = Path(OUTPUT_DIR) / \"landsat_stats.json\"\n",
    "        with open(stats_file, \"w\") as f:\n",
    "            json.dump(landsat_stats, f, indent=2)\n",
    "        print(f\"  ✓ Saved statistics to {stats_file}\")\n",
    "\n",
    "    fetched_count = 0\n",
    "    coordinates = []\n",
    "    \n",
    "    if csv_path.exists():\n",
    "        print(f\"\\n  Loading coordinates from CSV: {csv_path}\")\n",
    "        with open(csv_path, \"r\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            lat_col = next((col for col in reader.fieldnames if col.lower() in ['lat', 'latitude', 'y']), None)\n",
    "            lon_col = next((col for col in reader.fieldnames if col.lower() in ['lon', 'lng', 'longitude', 'x']), None)\n",
    "            \n",
    "            if lat_col and lon_col:\n",
    "                f.seek(0)\n",
    "                reader = csv.DictReader(f)\n",
    "                for row in reader:\n",
    "                    try:\n",
    "                        lat, lon = float(row[lat_col]), float(row[lon_col])\n",
    "                        b = BRAZIL_BOUNDS\n",
    "                        if not (b[0] < lon < b[1] and b[2] < lat < b[3]):\n",
    "                            continue\n",
    "                        coordinates.append({\"lat\": lat, \"lon\": lon})\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    \n",
    "    if coordinates:\n",
    "        print(f\"  Fetching {len(coordinates)} additional samples from CSV coordinates...\")\n",
    "        \n",
    "        try:\n",
    "            from satellite_fetcher import SatelliteFetcher\n",
    "            \n",
    "            \n",
    "            fetcher = SatelliteFetcher(date_range=DATE_RANGE, max_cloud_cover=MAX_CLOUD_COVER)\n",
    "            \n",
    "            for i, coord in enumerate(coordinates):\n",
    "                rgb, metadata = fetcher.fetch_image(\n",
    "                    lat=coord[\"lat\"], lon=coord[\"lon\"],\n",
    "                    distance_km=IMAGE_SIZE_KM, target_stats=landsat_stats\n",
    "                )\n",
    "                \n",
    "                if rgb is not None:\n",
    "                    filename = f\"mine_fetched_{fetched_count:04d}.jpg\"\n",
    "                    Image.fromarray(rgb).save(output_path / filename, quality=95)\n",
    "                    fetched_count += 1\n",
    "                    \n",
    "                if (i + 1) % 10 == 0:\n",
    "                    print(f\"    Processed {i + 1}/{len(coordinates)} ({fetched_count} successful)\")\n",
    "                \n",
    "                time.sleep(0.5)\n",
    "                \n",
    "        except ImportError as e:\n",
    "            print(f\"  ⚠ Could not fetch satellite images: {e}\")\n",
    "\n",
    "    result = {\n",
    "        \"output_dir\": str(output_path),\n",
    "        \"local_count\": len(local_images),\n",
    "        \"fetched_count\": fetched_count,\n",
    "        \"total_count\": len(local_images) + fetched_count,\n",
    "        \"landsat_stats\": landsat_stats,\n",
    "        \"image_size\": local_image_size\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  ✓ Total mine images collected: {result['total_count']}\")\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126100d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: Collect Forest Images (Negative Samples)\n",
    "# =============================================================================\n",
    "\n",
    "def step2_collect_forest(landsat_stats: Dict) -> Dict:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 2: Collecting Forest Images (Negative Samples)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    output_path = Path(OUTPUT_DIR) / \"raw_data\" / \"forest\"\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        from satellite_fetcher import SatelliteFetcher\n",
    "        \n",
    "        fetcher = SatelliteFetcher(date_range=DATE_RANGE, max_cloud_cover=MAX_CLOUD_COVER)\n",
    "        \n",
    "        print(f\"  Generating {N_FOREST_SAMPLES} forest samples...\")\n",
    "        \n",
    "        samples = fetcher.generate_forest_samples(\n",
    "            n_samples=N_FOREST_SAMPLES,\n",
    "            output_dir=str(output_path),\n",
    "            target_stats=landsat_stats,\n",
    "            brazil_bounds=BRAZIL_BOUNDS,\n",
    "            distance_km=IMAGE_SIZE_KM\n",
    "        )\n",
    "        \n",
    "        result = {\"output_dir\": str(output_path), \"count\": len(samples)}\n",
    "        print(f\"\\n  ✓ Generated {len(samples)} forest images\")\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"  Warning: Could not fetch forest images: {e}\")\n",
    "        result = {\"output_dir\": str(output_path), \"count\": 0}\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "helper functins for the extraction of statistics, matching distributions, and augmentation\n",
    "Dataset building: statistics extraction, distribution matching, and augmentation.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def match_image_to_stats(image: np.ndarray, target_stats: Dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Transform an image to match target statistics (Landsat style).\n",
    "    \n",
    "    Args:\n",
    "        image: RGB image as numpy array (uint8, 0-255)\n",
    "        target_stats: Statistics from extract_statistics()\n",
    "        \n",
    "    Returns:\n",
    "        Matched image as numpy array (uint8, 0-255)\n",
    "    \"\"\"\n",
    "    img = image.astype(np.float32) / 255.0\n",
    "    result = np.zeros_like(img)\n",
    "    \n",
    "    for i, channel in enumerate([\"r\", \"g\", \"b\"]):\n",
    "        src = img[:, :, i]\n",
    "        \n",
    "        # Get source percentiles\n",
    "        src_p2, src_p98 = np.percentile(src, [2, 98])\n",
    "        \n",
    "        # Get target percentiles\n",
    "        tgt_p2 = target_stats[f\"{channel}_p2\"]\n",
    "        tgt_p98 = target_stats[f\"{channel}_p98\"]\n",
    "        \n",
    "        # Map source to target distribution\n",
    "        if src_p98 > src_p2:\n",
    "            normalized = (src - src_p2) / (src_p98 - src_p2)\n",
    "            result[:, :, i] = normalized * (tgt_p98 - tgt_p2) + tgt_p2\n",
    "        else:\n",
    "            result[:, :, i] = target_stats[f\"{channel}_mean\"]\n",
    "    \n",
    "    return (np.clip(result, 0, 1) * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "class SatelliteAugmentation:\n",
    "    \"\"\"\n",
    "    Domain-appropriate augmentation for satellite imagery.\n",
    "    \n",
    "    Satellite images need:\n",
    "    - Only 90° rotations (not arbitrary angles)\n",
    "    - Atmospheric haze simulation\n",
    "    - Sensor noise\n",
    "    - No elastic deformation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, strength: str = \"medium\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            strength: \"light\", \"medium\", or \"strong\"\n",
    "        \"\"\"\n",
    "        self.strength = strength\n",
    "        \n",
    "        # Parameters per strength level\n",
    "        self.params = {\n",
    "            \"light\": {\n",
    "                \"brightness_range\": (0.95, 1.05),\n",
    "                \"contrast_range\": (0.95, 1.05),\n",
    "                \"noise_std\": 0.01,\n",
    "                \"haze_prob\": 0.1,\n",
    "                \"haze_strength\": 0.05\n",
    "            },\n",
    "            \"medium\": {\n",
    "                \"brightness_range\": (0.85, 1.15),\n",
    "                \"contrast_range\": (0.85, 1.15),\n",
    "                \"noise_std\": 0.02,\n",
    "                \"haze_prob\": 0.3,\n",
    "                \"haze_strength\": 0.1\n",
    "            },\n",
    "            \"strong\": {\n",
    "                \"brightness_range\": (0.7, 1.3),\n",
    "                \"contrast_range\": (0.7, 1.3),\n",
    "                \"noise_std\": 0.03,\n",
    "                \"haze_prob\": 0.5,\n",
    "                \"haze_strength\": 0.15\n",
    "            }\n",
    "        }[strength]\n",
    "    \n",
    "    def augment(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply random augmentation to an image.\"\"\"\n",
    "        img = image.astype(np.float32) / 255.0\n",
    "        \n",
    "        # 1. Random 90° rotation\n",
    "        k = random.randint(0, 3)\n",
    "        img = np.rot90(img, k)\n",
    "        \n",
    "        # 2. Random flip\n",
    "        if random.random() < 0.5:\n",
    "            img = np.fliplr(img)\n",
    "        if random.random() < 0.5:\n",
    "            img = np.flipud(img)\n",
    "        \n",
    "        # 3. Brightness adjustment (sun angle simulation)\n",
    "        brightness = random.uniform(*self.params[\"brightness_range\"])\n",
    "        img = img * brightness\n",
    "        \n",
    "        # 4. Contrast adjustment\n",
    "        contrast = random.uniform(*self.params[\"contrast_range\"])\n",
    "        mean = img.mean()\n",
    "        img = (img - mean) * contrast + mean\n",
    "        \n",
    "        # 5. Atmospheric haze\n",
    "        if random.random() < self.params[\"haze_prob\"]:\n",
    "            haze = self.params[\"haze_strength\"] * random.random()\n",
    "            # Haze is more visible in darker areas\n",
    "            luminance = 0.299 * img[:, :, 0] + 0.587 * img[:, :, 1] + 0.114 * img[:, :, 2]\n",
    "            haze_mask = 1 - luminance\n",
    "            for i in range(3):\n",
    "                img[:, :, i] = img[:, :, i] + haze * haze_mask\n",
    "        \n",
    "        # 6. Sensor noise\n",
    "        noise = np.random.normal(0, self.params[\"noise_std\"], img.shape)\n",
    "        img = img + noise\n",
    "        \n",
    "        return (np.clip(img, 0, 1) * 255).astype(np.uint8)\n",
    "    \n",
    "    def augment_batch(self, image: np.ndarray, n: int) -> List[np.ndarray]:\n",
    "        \"\"\"Generate n augmented versions of an image.\"\"\"\n",
    "        return [self.augment(image) for _ in range(n)]\n",
    "\n",
    "\n",
    "def build_dataset(\n",
    "    positive_dir: str,\n",
    "    negative_dir: str,\n",
    "    output_dir: str,\n",
    "    target_stats: Optional[Dict] = None,\n",
    "    augmentation_strength: str = \"medium\",\n",
    "    n_augmented_per_image: int = 5,\n",
    "    match_distributions: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Build a complete training dataset with augmentation.\n",
    "    \n",
    "    Args:\n",
    "        positive_dir: Directory with mine images\n",
    "        negative_dir: Directory with forest images\n",
    "        output_dir: Output directory\n",
    "        target_stats: If provided, match all images to these stats\n",
    "        augmentation_strength: \"light\", \"medium\", \"strong\"\n",
    "        n_augmented_per_image: Number of augmented copies per image\n",
    "        match_distributions: Whether to match negative to positive stats\n",
    "        \n",
    "    Returns:\n",
    "        Dataset metadata\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    # Create output directories\n",
    "    (output_path / \"positive\").mkdir(parents=True, exist_ok=True)\n",
    "    (output_path / \"negative\").mkdir(parents=True, exist_ok=True)\n",
    "    (output_path / \"positive_augmented\").mkdir(parents=True, exist_ok=True)\n",
    "    (output_path / \"negative_augmented\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get statistics from positive images if not provided\n",
    "    if target_stats is None and match_distributions:\n",
    "        print(\"  Extracting statistics from positive images...\")\n",
    "        target_stats = extract_statistics(positive_dir)\n",
    "    \n",
    "    augmenter = SatelliteAugmentation(strength=augmentation_strength)\n",
    "    \n",
    "    metadata = {\n",
    "        \"positive_original\": 0,\n",
    "        \"negative_original\": 0,\n",
    "        \"positive_augmented\": 0,\n",
    "        \"negative_augmented\": 0,\n",
    "        \"target_stats\": target_stats\n",
    "    }\n",
    "    \n",
    "    # Process positive images\n",
    "    print(\"  Processing positive (mine) images...\")\n",
    "    pos_files = list(Path(positive_dir).glob(\"*.jpg\")) + list(Path(positive_dir).glob(\"*.png\"))\n",
    "    \n",
    "    for i, img_file in enumerate(pos_files):\n",
    "        img = np.array(Image.open(img_file).convert(\"RGB\"))\n",
    "        \n",
    "        # Save original\n",
    "        out_name = f\"pos_{i:04d}.jpg\"\n",
    "        Image.fromarray(img).save(output_path / \"positive\" / out_name, quality=95)\n",
    "        metadata[\"positive_original\"] += 1\n",
    "        \n",
    "        # Save augmented versions (one at a time to save memory)\n",
    "        for j in range(n_augmented_per_image):\n",
    "            aug_img = augmenter.augment(img)\n",
    "            aug_name = f\"pos_{i:04d}_aug{j:02d}.jpg\"\n",
    "            Image.fromarray(aug_img).save(output_path / \"positive_augmented\" / aug_name, quality=95)\n",
    "            metadata[\"positive_augmented\"] += 1\n",
    "            del aug_img\n",
    "        \n",
    "        del img  # Free memory\n",
    "    \n",
    "    # Process negative images\n",
    "    print(\"  Processing negative (forest) images...\")\n",
    "    neg_files = list(Path(negative_dir).glob(\"*.jpg\")) + list(Path(negative_dir).glob(\"*.png\"))\n",
    "    \n",
    "    for i, img_file in enumerate(neg_files):\n",
    "        img = np.array(Image.open(img_file).convert(\"RGB\"))\n",
    "        \n",
    "        # Match to positive statistics if requested\n",
    "        if match_distributions and target_stats:\n",
    "            img = match_image_to_stats(img, target_stats)\n",
    "        \n",
    "        # Save original (matched)\n",
    "        out_name = f\"neg_{i:04d}.jpg\"\n",
    "        Image.fromarray(img).save(output_path / \"negative\" / out_name, quality=95)\n",
    "        metadata[\"negative_original\"] += 1\n",
    "        \n",
    "        # Save augmented versions (one at a time to save memory)\n",
    "        for j in range(n_augmented_per_image):\n",
    "            aug_img = augmenter.augment(img)\n",
    "            aug_name = f\"neg_{i:04d}_aug{j:02d}.jpg\"\n",
    "            Image.fromarray(aug_img).save(output_path / \"negative_augmented\" / aug_name, quality=95)\n",
    "            metadata[\"negative_augmented\"] += 1\n",
    "            del aug_img\n",
    "        \n",
    "        del img  # Free memory\n",
    "    \n",
    "    # Save metadata\n",
    "    # Convert stats to JSON-serializable format\n",
    "    json_metadata = {k: v for k, v in metadata.items() if k != \"target_stats\"}\n",
    "    if target_stats:\n",
    "        json_metadata[\"target_stats\"] = target_stats\n",
    "    \n",
    "    with open(output_path / \"dataset_metadata.json\", \"w\") as f:\n",
    "        json.dump(json_metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"  Dataset built: {metadata['positive_original']} positive, {metadata['negative_original']} negative\")\n",
    "    print(f\"  With augmentation: {metadata['positive_augmented']} + {metadata['negative_augmented']} additional\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "def merge_image_directories(dirs: List[str], output_dir: str) -> int:\n",
    "    \"\"\"\n",
    "    Merge multiple image directories into one.\n",
    "    \n",
    "    Args:\n",
    "        dirs: List of source directories\n",
    "        output_dir: Output directory\n",
    "        \n",
    "    Returns:\n",
    "        Number of images copied\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    count = 0\n",
    "    for src_dir in dirs:\n",
    "        src_path = Path(src_dir)\n",
    "        if not src_path.exists():\n",
    "            print(f\"  Warning: {src_dir} does not exist, skipping\")\n",
    "            continue\n",
    "        \n",
    "        for img_file in list(src_path.glob(\"*.jpg\")) + list(src_path.glob(\"*.png\")):\n",
    "            dst_name = f\"img_{count:04d}{img_file.suffix}\"\n",
    "            shutil.copy(img_file, output_path / dst_name)\n",
    "            count += 1\n",
    "    \n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fde29d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: Build Training Dataset\n",
    "# =============================================================================\n",
    "\n",
    "def step3_build_dataset(mines_dir: str, forest_dir: str, landsat_stats: Dict) -> Dict:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 3: Building Training Dataset (Split First -> Augment Train Only)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    output_path = Path(OUTPUT_DIR) / \"dataset\"\n",
    "    \n",
    "\n",
    "    metadata = build_dataset(\n",
    "        positive_dir=mines_dir,\n",
    "        negative_dir=forest_dir,\n",
    "        output_dir=str(output_path),\n",
    "        target_stats=landsat_stats,\n",
    "        augmentation_strength=AUGMENTATION_STRENGTH,\n",
    "        n_augmented_per_image=N_AUGMENTED_PER_IMAGE,\n",
    "        match_distributions=MATCH_TO_LANDSAT_STYLE,\n",
    "        val_split=VALIDATION_SPLIT \n",
    "    )\n",
    "    \n",
    "    metadata[\"output_dir\"] = str(output_path)\n",
    "    print(f\"\\n  ✓ Dataset built at {output_path}\")\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001f92bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN classifier for mining detection.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class MiningDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for mining classification.\n",
    "    Loads from a specific split directory ('train' or 'val').\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_dir: str,\n",
    "        split: str = \"train\",  # Added split argument\n",
    "        image_size: int = 224\n",
    "    ):\n",
    "        self.image_size = image_size\n",
    "        self.split = split\n",
    "        \n",
    "        # Standard normalization for both train and val\n",
    "        # (Augmentation is now done offline in build_dataset)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        root_path = Path(dataset_dir) / split\n",
    "        if not root_path.exists():\n",
    "            raise ValueError(f\"Dataset split directory not found: {root_path}\")\n",
    "            \n",
    "        self.samples = []\n",
    "        \n",
    "        # Load positive samples (Label: 1)\n",
    "        pos_dir = root_path / \"positive\"\n",
    "        if pos_dir.exists():\n",
    "            for img_file in pos_dir.glob(\"*.jpg\"):\n",
    "                self.samples.append((str(img_file), 1.0))\n",
    "        \n",
    "        # Load negative samples (Label: 0)\n",
    "        neg_dir = root_path / \"negative\"\n",
    "        if neg_dir.exists():\n",
    "            for img_file in neg_dir.glob(\"*.jpg\"):\n",
    "                self.samples.append((str(img_file), 0.0))\n",
    "                \n",
    "        if len(self.samples) == 0:\n",
    "            raise ValueError(f\"No images found in {root_path}\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img_tensor = self.transform(img)\n",
    "            return img_tensor, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            # Return a dummy tensor in case of corruption, or handle appropriately\n",
    "            return torch.zeros((3, self.image_size, self.image_size)), label\n",
    "\n",
    "\n",
    "\n",
    "class MiningClassifier(nn.Module):\n",
    "    \"\"\"CNN classifier with pretrained backbone.\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone: str = \"resnet18\", pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if backbone == \"resnet18\":\n",
    "            self.backbone = models.resnet18(pretrained=pretrained)\n",
    "            num_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Sequential(\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(num_features, 1)\n",
    "            )\n",
    "        elif backbone == \"resnet34\":\n",
    "            self.backbone = models.resnet34(pretrained=pretrained)\n",
    "            num_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Sequential(\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(num_features, 1)\n",
    "            )\n",
    "        elif backbone == \"efficientnet_b0\":\n",
    "            self.backbone = models.efficientnet_b0(pretrained=pretrained)\n",
    "            num_features = self.backbone.classifier[1].in_features\n",
    "            self.backbone.classifier = nn.Sequential(\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(num_features, 1)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backbone: {backbone}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.backbone(x))\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    dataset_dir: str,\n",
    "    output_dir: str,\n",
    "    backbone: str = \"resnet18\",\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 1e-4,\n",
    "    epochs: int = 30,\n",
    "    patience: int = 7,\n",
    "    image_size: int = 224\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Train a mining classifier using pre-split folders.\n",
    "    \"\"\"\n",
    "    if not HAS_TORCH:\n",
    "        raise ImportError(\"PyTorch required for training\")\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"  Using device: {device}\")\n",
    "    \n",
    "    # 1. Load Datasets (Physical Split)\n",
    "    print(\"  Loading Training Set...\")\n",
    "    train_dataset = MiningDataset(dataset_dir, split=\"train\", image_size=image_size)\n",
    "    \n",
    "    print(\"  Loading Validation Set...\")\n",
    "    val_dataset = MiningDataset(dataset_dir, split=\"val\", image_size=image_size)\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_dataset)}\")\n",
    "    print(f\"  Val samples:   {len(val_dataset)}\")\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # 2. Model Setup\n",
    "    model = MiningClassifier(backbone=backbone).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5, verbose=True)\n",
    "    \n",
    "    # 3. Training Loop\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().to(device).unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        train_loss /= len(train_dataset)\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.float().to(device).unsqueeze(1)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                \n",
    "                predicted = (outputs > 0.5).float()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_dataset)\n",
    "        val_acc = correct / total\n",
    "        \n",
    "        # Scheduler Step\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Logging\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        \n",
    "        print(f\"  Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Early Stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), output_path / \"best_model.pth\")\n",
    "            # print(\"    -> Saved best model\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"  Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Save Final\n",
    "    torch.save(model.state_dict(), output_path / \"final_model.pth\")\n",
    "    \n",
    "    with open(output_path / \"training_history.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "    \"\"\"Run inference with a trained model.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        backbone: str = \"resnet18\",\n",
    "        image_size: int = 224,\n",
    "        threshold: float = 0.5\n",
    "    ):\n",
    "        if not HAS_TORCH:\n",
    "            raise ImportError(\"PyTorch required for inference\")\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.threshold = threshold\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Load model\n",
    "        self.model = MiningClassifier(backbone=backbone, pretrained=False)\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Transform\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def predict(self, image_path: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Predict on a single image.\n",
    "        \n",
    "        Returns:\n",
    "            {\"probability\": float, \"prediction\": str, \"is_mining\": bool}\n",
    "        \"\"\"\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        img_tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prob = self.model(img_tensor).item()\n",
    "        \n",
    "        return {\n",
    "            \"probability\": prob,\n",
    "            \"prediction\": \"mining\" if prob > self.threshold else \"forest\",\n",
    "            \"is_mining\": prob > self.threshold\n",
    "        }\n",
    "    \n",
    "    def predict_array(self, image: np.ndarray) -> Dict:\n",
    "        \"\"\"Predict on a numpy array (RGB, 0-255).\"\"\"\n",
    "        img = Image.fromarray(image)\n",
    "        img_tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prob = self.model(img_tensor).item()\n",
    "        \n",
    "        return {\n",
    "            \"probability\": prob,\n",
    "            \"prediction\": \"mining\" if prob > self.threshold else \"forest\",\n",
    "            \"is_mining\": prob > self.threshold\n",
    "        }\n",
    "    \n",
    "    def predict_batch(self, image_dir: str) -> List[Dict]:\n",
    "        \"\"\"Predict on all images in a directory.\"\"\"\n",
    "        results = []\n",
    "        image_path = Path(image_dir)\n",
    "        \n",
    "        for img_file in list(image_path.glob(\"*.jpg\")) + list(image_path.glob(\"*.png\")):\n",
    "            result = self.predict(str(img_file))\n",
    "            result[\"filename\"] = img_file.name\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261c16bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: Train Model\n",
    "# =============================================================================\n",
    "\n",
    "def step4_train(dataset_dir: str) -> Dict:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 4: Training Classifier\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    output_path = Path(OUTPUT_DIR) / \"model\"\n",
    "    \n",
    "    history = train_model(\n",
    "        dataset_dir=dataset_dir,\n",
    "        output_dir=str(output_path),\n",
    "        backbone=BACKBONE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        epochs=EPOCHS,\n",
    "        patience=EARLY_STOPPING_PATIENCE,\n",
    "        image_size=MODEL_IMAGE_SIZE\n",
    "    )\n",
    "    \n",
    "    history[\"model_dir\"] = str(output_path)\n",
    "    print(f\"\\n  ✓ Model saved to {output_path}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e4990",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MiningSegmentationDetector:\n",
    "    \"\"\"\n",
    "    Robust SegFormer detector.\n",
    "    Identifies 'suspicious' pixels including industrial/disturbed land.\n",
    "    \"\"\"\n",
    "    \n",
    "    # LoveDA Class Labels:\n",
    "    # 0:Background, 1:Building, 2:Road, 3:Water, 4:Barren, 5:Forest, 6:Agricultural\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"wu-pr-gw/segformer-b2-finetuned-with-LoveDA\",\n",
    "        device: Optional[str] = None\n",
    "    ):\n",
    "        if not HAS_SEGFORMER:\n",
    "            raise ImportError(\"transformers required: pip install transformers\")\n",
    "        \n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Loading SegFormer: {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            self.processor = SegformerImageProcessor.from_pretrained(model_name)\n",
    "            self.model = SegformerForSemanticSegmentation.from_pretrained(model_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: {e}. Fallback to generic model.\")\n",
    "            fallback = \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    "            self.processor = SegformerImageProcessor.from_pretrained(fallback)\n",
    "            self.model = SegformerForSemanticSegmentation.from_pretrained(fallback)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "       # clsses are Building, Road, Water, Barren\n",
    "        self.suspicious_classes = {1, 2, 3, 4} \n",
    "\n",
    "    def predict_mask(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Returns the raw class ID mask (H, W).\"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        max_dim = 2048\n",
    "        if max(h, w) > max_dim:\n",
    "            scale = max_dim / max(h, w)\n",
    "            new_w, new_h = int(w * scale), int(h * scale)\n",
    "            img_input = cv2.resize(image, (new_w, new_h))\n",
    "        else:\n",
    "            img_input = image\n",
    "\n",
    "        inputs = self.processor(images=img_input, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = torch.nn.functional.interpolate(\n",
    "                outputs.logits, size=(h, w), mode=\"bilinear\", align_corners=False\n",
    "            )\n",
    "            pred_mask = logits.argmax(dim=1).squeeze().cpu().numpy()\n",
    "            \n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return pred_mask.astype(np.uint8)\n",
    "\n",
    "    def get_suspicious_mask(self, raw_mask: np.ndarray, smooth: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Converts raw class mask to a binary 'Suspicious' mask.\n",
    "        \"\"\"\n",
    "        suspicious = np.isin(raw_mask, list(self.suspicious_classes)).astype(np.uint8) * 255\n",
    "        \n",
    "        if not smooth:\n",
    "            return suspicious\n",
    "\n",
    "        # BALANCED SMOOTHING (5x5)\n",
    "        # Big enough to remove salt-and-pepper noise.\n",
    "        # Small enough to keep mines distinct from towns.\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "        smoothed = cv2.morphologyEx(suspicious, cv2.MORPH_CLOSE, kernel)\n",
    "        smoothed = cv2.morphologyEx(smoothed, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        return smoothed\n",
    "\n",
    "class DetectionVisualizer:\n",
    "    CLASS_COLORS = np.array([\n",
    "        [0, 0, 0],       # 0: Background\n",
    "        [255, 0, 0],     # 1: Building (Red)\n",
    "        [255, 215, 0],   # 2: Road (Gold)\n",
    "        [0, 0, 255],     # 3: Water (Blue)\n",
    "        [160, 82, 45],   # 4: Barren (Brown)\n",
    "        [34, 139, 34],   # 5: Forest (Green)\n",
    "        [154, 205, 50]   # 6: Agri (LtGreen)\n",
    "    ], dtype=np.uint8)\n",
    "\n",
    "    def draw_segmentation_overlay(self, image: np.ndarray, mask: np.ndarray, alpha: float = 0.5) -> np.ndarray:\n",
    "        safe_mask = np.clip(mask, 0, len(self.CLASS_COLORS) - 1)\n",
    "        colored_mask = self.CLASS_COLORS[safe_mask]\n",
    "        return (image * (1 - alpha) + colored_mask * alpha).astype(np.uint8)\n",
    "    \n",
    "    def draw_suspicious_overlay(self, image: np.ndarray, binary_mask: np.ndarray) -> np.ndarray:\n",
    "        overlay = image.copy()\n",
    "        overlay[binary_mask > 0] = [255, 0, 0] # Red highlight\n",
    "        return cv2.addWeighted(image, 0.7, overlay, 0.3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05101980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 5: Two-Stage Detection (Center + Radius)\n",
    "# =============================================================================\n",
    "\n",
    "def pixel_to_geo(pixel_coords: tuple, image_size: tuple, image_bounds: tuple) -> tuple:\n",
    "    \"\"\"Convert pixel coordinates to geographic coordinates.\"\"\"\n",
    "    x, y = pixel_coords\n",
    "    w, h = image_size\n",
    "    lon_min, lat_min, lon_max, lat_max = image_bounds\n",
    "    \n",
    "    lon = lon_min + (x / w) * (lon_max - lon_min)\n",
    "    lat = lat_max - (y / h) * (lat_max - lat_min)\n",
    "    \n",
    "    return (lon, lat)\n",
    "\n",
    "\n",
    "def bbox_to_geo(bbox: tuple, image_size: tuple, image_bounds: tuple) -> dict:\n",
    "    \"\"\"Convert pixel bounding box to geographic coordinates.\"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    w, h = image_size\n",
    "    lon_min, lat_min, lon_max, lat_max = image_bounds\n",
    "    \n",
    "    geo_lon_min = lon_min + (x1 / w) * (lon_max - lon_min)\n",
    "    geo_lon_max = lon_min + (x2 / w) * (lon_max - lon_min)\n",
    "    geo_lat_max = lat_max - (y1 / h) * (lat_max - lat_min)\n",
    "    geo_lat_min = lat_max - (y2 / h) * (lat_max - lat_min)\n",
    "    \n",
    "    return {\n",
    "        \"lon_min\": geo_lon_min, \"lon_max\": geo_lon_max,\n",
    "        \"lat_min\": geo_lat_min, \"lat_max\": geo_lat_max,\n",
    "        \"centroid_lon\": (geo_lon_min + geo_lon_max) / 2,\n",
    "        \"centroid_lat\": (geo_lat_min + geo_lat_max) / 2\n",
    "    }\n",
    "\n",
    "\n",
    "def step5_detect_overview(\n",
    "    model_dir: str,\n",
    "    center_lat: Optional[float] = None,\n",
    "    center_lon: Optional[float] = None,\n",
    "    radius_km: float = OVERVIEW_RADIUS_KM,\n",
    "    overview_max_dimension: int = 2048,\n",
    "    classification_distance_km: float = 2.5,\n",
    "    landsat_stats: Optional[Dict] = None,\n",
    "    rate_limit_seconds: float = 0.5\n",
    ") -> Dict:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 5: Grid-Based Detection (Restored Classes)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    model_path = Path(model_dir) / \"best_model.pth\"\n",
    "    if not model_path.exists(): return {\"error\": \"Classifier model not found\"}\n",
    "    \n",
    "    try:\n",
    "        from segmentation_detector import MiningSegmentationDetector, DetectionVisualizer\n",
    "        from satellite_fetcher import SatelliteFetcher, center_to_bbox\n",
    "        from classifier import Predictor\n",
    "        from PIL import ImageDraw\n",
    "        import torch\n",
    "    except ImportError as e: return {\"error\": str(e)}\n",
    "\n",
    "    # 1. Fetch Overview\n",
    "    print(\"\\n  [Stage 1] Fetching Overview...\")\n",
    "    fetcher = SatelliteFetcher(date_range=DATE_RANGE, max_cloud_cover=MAX_CLOUD_COVER)\n",
    "    overview_image, overview_metadata = fetcher.fetch_overview(\n",
    "        center_lat=center_lat, center_lon=center_lon,\n",
    "        radius_km=radius_km, max_dimension=overview_max_dimension,\n",
    "        target_stats=landsat_stats\n",
    "    )\n",
    "    if overview_image is None: return {\"error\": \"Failed to fetch overview\"}\n",
    "\n",
    "    output_path = Path(OUTPUT_DIR) / \"detections\"\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    Image.fromarray(overview_image).save(output_path / \"overview_raw.jpg\")\n",
    "\n",
    "    # 2. Run Global Segmentation\n",
    "    print(\"  Running Global Segmentation...\")\n",
    "    detector = MiningSegmentationDetector(model_name=\"wu-pr-gw/segformer-b2-finetuned-with-LoveDA\")\n",
    "    visualizer = DetectionVisualizer()\n",
    "\n",
    "    # Get Masks\n",
    "    raw_mask = detector.predict_mask(overview_image)\n",
    "    suspicious_mask = detector.get_suspicious_mask(raw_mask, smooth=True)\n",
    "\n",
    "    # Save Visualizations\n",
    "    color_overlay = visualizer.draw_segmentation_overlay(overview_image, raw_mask)\n",
    "    Image.fromarray(color_overlay).save(output_path / \"overview_segmentation_classes.jpg\")\n",
    "    \n",
    "    suspicious_overlay = visualizer.draw_suspicious_overlay(overview_image, suspicious_mask)\n",
    "    Image.fromarray(suspicious_overlay).save(output_path / \"overview_segmentation_suspicious.jpg\")\n",
    "    print(f\"  Saved segmentations to {output_path}\")\n",
    "\n",
    "    # 3. Process Grid\n",
    "    h, w = overview_image.shape[:2]\n",
    "    total_width_km = radius_km * 2\n",
    "    grid_n = max(2, int(total_width_km // 2.5)) \n",
    "    step_y, step_x = h // grid_n, w // grid_n\n",
    "    \n",
    "    print(f\"\\n  [Stage 2] Processing {grid_n}x{grid_n} Grid...\")\n",
    "    classifier = Predictor(model_path=str(model_path), backbone=BACKBONE, image_size=MODEL_IMAGE_SIZE)\n",
    "    results = []\n",
    "    \n",
    "    viz_img = Image.fromarray(overview_image).convert(\"RGBA\")\n",
    "    draw = ImageDraw.Draw(viz_img)\n",
    "    bbox = center_to_bbox(center_lat, center_lon, radius_km)\n",
    "    \n",
    "    classification_dir = output_path / \"classification_crops\"\n",
    "    classification_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for r in range(grid_n):\n",
    "        for c in range(grid_n):\n",
    "            y1, x1 = r * step_y, c * step_x\n",
    "            y2 = h if r == grid_n - 1 else (r + 1) * step_y\n",
    "            x2 = w if c == grid_n - 1 else (c + 1) * step_x\n",
    "            \n",
    "            # Analyze Mask\n",
    "            cell_mask = suspicious_mask[y1:y2, x1:x2]\n",
    "            suspicious_pixels = np.count_nonzero(cell_mask)\n",
    "            suspicious_ratio = suspicious_pixels / cell_mask.size\n",
    "            \n",
    "            # Draw Grid\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=(255, 255, 255, 80), width=1)\n",
    "            \n",
    "            # DEBUG PRINT\n",
    "            # print(f\"    Grid ({r},{c}): Ratio {suspicious_ratio:.1%}\")\n",
    "\n",
    "            # Threshold > 5%\n",
    "            if suspicious_ratio > 0.05:\n",
    "                cy_px, cx_px = (y1 + y2) // 2, (x1 + x2) // 2\n",
    "                cell_lat = bbox[3] - (cy_px / h) * (bbox[3] - bbox[1])\n",
    "                cell_lon = bbox[0] + (cx_px / w) * (bbox[2] - bbox[0])\n",
    "                \n",
    "                rgb, _ = fetcher.fetch_image(\n",
    "                    lat=cell_lat, lon=cell_lon,\n",
    "                    distance_km=classification_distance_km,\n",
    "                    target_stats=landsat_stats,\n",
    "                    simulate_landsat=True\n",
    "                )\n",
    "                \n",
    "                if rgb is not None:\n",
    "                    # Save Crop\n",
    "                    crop_filename = f\"cell_{r}_{c}_{suspicious_ratio:.2f}.jpg\"\n",
    "                    crop_path = classification_dir / crop_filename\n",
    "                    Image.fromarray(rgb).save(crop_path)\n",
    "                    \n",
    "                    # Classify\n",
    "                    pred = classifier.predict(str(crop_path))\n",
    "                    \n",
    "                    if pred[\"is_mining\"]:\n",
    "                        draw.rectangle([x1, y1, x2, y2], outline=(255, 0, 0), width=4)\n",
    "                        results.append({\"lat\": cell_lat, \"lon\": cell_lon, \"prob\": pred[\"probability\"], \"type\": \"mining\"})\n",
    "                        print(f\"    Cell ({r},{c}): 🚨 MINING ({pred['probability']:.1%}) - Suspicious Area: {suspicious_ratio:.1%}\")\n",
    "                    else:\n",
    "                        draw.rectangle([x1, y1, x2, y2], outline=(0, 255, 0), width=2)\n",
    "                        print(f\"    Cell ({r},{c}): Clean ({pred['probability']:.1%}) - Suspicious Area: {suspicious_ratio:.1%}\")\n",
    "                else:\n",
    "                    print(f\"    Cell ({r},{c}): Fetch Failed\")\n",
    "            else:\n",
    "                pass # Skip\n",
    "\n",
    "    viz_img.convert(\"RGB\").save(output_path / \"grid_analysis.jpg\")\n",
    "    print(f\"\\n  Saved visualization to {output_path / 'grid_analysis.jpg'}\")\n",
    "    \n",
    "    return {\"results\": results}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495685ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 6: Validate Model Against Known Coordinates\n",
    "# =============================================================================\n",
    "\n",
    "def step6_validate(\n",
    "    model_dir: str,\n",
    "    validation_csv: Optional[str] = None,\n",
    "    known_coordinates: Optional[List[Dict]] = None,\n",
    "    sample_size: Optional[int] = None\n",
    ") -> Dict:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 6: Validating Model Against Known Coordinates\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    model_path = Path(model_dir) / \"best_model.pth\"\n",
    "    \n",
    "    if not model_path.exists():\n",
    "        print(f\"  Error: Model not found at {model_path}\")\n",
    "        return {\"error\": \"Model not found\"}\n",
    "    \n",
    "    coordinates = []\n",
    "    \n",
    "    if known_coordinates:\n",
    "        coordinates = known_coordinates\n",
    "    elif validation_csv and Path(validation_csv).exists():\n",
    "        print(f\"  Loading validation data from: {validation_csv}\")\n",
    "        with open(validation_csv, \"r\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    lat_col = next((col for col in row.keys() if col.lower() in ['lat', 'latitude']), None)\n",
    "                    lon_col = next((col for col in row.keys() if col.lower() in ['lon', 'lng', 'longitude']), None)\n",
    "                    label_col = next((col for col in row.keys() if col.lower() in ['label', 'class', 'type']), None)\n",
    "                    \n",
    "                    if lat_col and lon_col and label_col:\n",
    "                        coordinates.append({\n",
    "                            \"lat\": float(row[lat_col]),\n",
    "                            \"lon\": float(row[lon_col]),\n",
    "                            \"label\": row[label_col].lower()\n",
    "                        })\n",
    "                except (ValueError, KeyError):\n",
    "                    continue\n",
    "    \n",
    "    if not coordinates:\n",
    "        print(\"  No validation coordinates provided. Skipping validation.\")\n",
    "        return {\"skipped\": True}\n",
    "    \n",
    "    if sample_size and len(coordinates) > sample_size:\n",
    "        import random\n",
    "        coordinates = random.sample(coordinates, sample_size)\n",
    "    \n",
    "    print(f\"  Validating on {len(coordinates)} samples\")\n",
    "    \n",
    "    output_path = Path(OUTPUT_DIR) / \"validation\"\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    images_dir = output_path / \"images\"\n",
    "    images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        from satellite_fetcher import SatelliteFetcher\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "    \n",
    "    fetcher = SatelliteFetcher(date_range=DATE_RANGE, max_cloud_cover=MAX_CLOUD_COVER)\n",
    "    classifier = Predictor(model_path=str(model_path), backbone=BACKBONE, image_size=MODEL_IMAGE_SIZE, threshold=MINING_THRESHOLD)\n",
    "    \n",
    "    results = []\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "    \n",
    "    for i, coord in enumerate(coordinates):\n",
    "        rgb, metadata = fetcher.fetch_image(lat=coord[\"lat\"], lon=coord[\"lon\"], distance_km=IMAGE_SIZE_KM)\n",
    "        \n",
    "        if rgb is None:\n",
    "            results.append({\"lat\": coord[\"lat\"], \"lon\": coord[\"lon\"], \"known_label\": coord[\"label\"], \"predicted\": None, \"correct\": None, \"error\": metadata.get(\"error\")})\n",
    "            continue\n",
    "        \n",
    "        img_path = images_dir / f\"val_{i:04d}_{coord['label']}.jpg\"\n",
    "        Image.fromarray(rgb).save(img_path, quality=95)\n",
    "        \n",
    "        # Free rgb after saving\n",
    "        del rgb\n",
    "        \n",
    "        pred = classifier.predict(str(img_path))\n",
    "        predicted_label = \"mining\" if pred[\"is_mining\"] else \"forest\"\n",
    "        known_is_mining = coord[\"label\"] in [\"mining\", \"mine\", \"positive\", \"1\", \"true\"]\n",
    "        \n",
    "        correct = (pred[\"is_mining\"] == known_is_mining)\n",
    "        \n",
    "        if known_is_mining and pred[\"is_mining\"]: tp += 1\n",
    "        elif known_is_mining and not pred[\"is_mining\"]: fn += 1\n",
    "        elif not known_is_mining and pred[\"is_mining\"]: fp += 1\n",
    "        else: tn += 1\n",
    "        \n",
    "        results.append({\"lat\": coord[\"lat\"], \"lon\": coord[\"lon\"], \"known_label\": coord[\"label\"], \"predicted\": predicted_label, \"probability\": pred[\"probability\"], \"correct\": correct, \"image_path\": str(img_path)})\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"    Processed {i + 1}/{len(coordinates)}\")\n",
    "            gc.collect()\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    total = tp + tn + fp + fn\n",
    "    metrics = {\n",
    "        \"total\": len(coordinates), \"processed\": total,\n",
    "        \"accuracy\": (tp + tn) / total if total > 0 else 0,\n",
    "        \"precision\": tp / (tp + fp) if (tp + fp) > 0 else 0,\n",
    "        \"recall\": tp / (tp + fn) if (tp + fn) > 0 else 0,\n",
    "        \"f1\": 2 * (tp / (tp + fp)) * (tp / (tp + fn)) / ((tp / (tp + fp)) + (tp / (tp + fn))) if (tp + fp) > 0 and (tp + fn) > 0 else 0,\n",
    "        \"true_positives\": tp, \"false_positives\": fp, \"true_negatives\": tn, \"false_negatives\": fn\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  VALIDATION RESULTS\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.1%}, Precision: {metrics['precision']:.1%}, Recall: {metrics['recall']:.1%}, F1: {metrics['f1']:.3f}\")\n",
    "    \n",
    "    output = {\"metrics\": metrics, \"results\": results}\n",
    "    \n",
    "    results_path = output_path / \"validation_results.json\"\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cd00b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    skip_data_collection: bool = False,\n",
    "    skip_training: bool = False,\n",
    "    center_lat: Optional[float] = None,\n",
    "    center_lon: Optional[float] = None,\n",
    "    radius_km: float = OVERVIEW_RADIUS_KM,\n",
    "    overview_max_dimension: int = 2048,\n",
    "    classification_distance_km: float = 2.5,\n",
    "    validation_csv: Optional[str] = None,\n",
    "    known_coordinates: Optional[List[Dict]] = None\n",
    "):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ILLEGAL MINING DETECTION PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "    \n",
    "    outputs = {}\n",
    "    model_dir = Path(OUTPUT_DIR) / \"model\"\n",
    "    landsat_stats = None\n",
    "    \n",
    "    if not skip_training:\n",
    "        if not skip_data_collection:\n",
    "            outputs[\"step1\"] = step1_collect_mines()\n",
    "            landsat_stats = outputs[\"step1\"][\"landsat_stats\"]\n",
    "            outputs[\"step2\"] = step2_collect_forest(landsat_stats=landsat_stats)\n",
    "            outputs[\"step3\"] = step3_build_dataset(\n",
    "                mines_dir=outputs[\"step1\"][\"output_dir\"],\n",
    "                forest_dir=outputs[\"step2\"][\"output_dir\"],\n",
    "                landsat_stats=landsat_stats\n",
    "            )\n",
    "        else:\n",
    "            print(\"Skipping data collection (steps 1-2)\")\n",
    "            outputs[\"step3\"] = {\"output_dir\": str(Path(OUTPUT_DIR) / \"dataset\")}\n",
    "            stats_file = Path(OUTPUT_DIR) / \"landsat_stats.json\"\n",
    "            if stats_file.exists():\n",
    "                with open(stats_file, \"r\") as f:\n",
    "                    landsat_stats = json.load(f)\n",
    "        \n",
    "        outputs[\"step4\"] = step4_train(dataset_dir=outputs[\"step3\"][\"output_dir\"])\n",
    "        model_dir = outputs[\"step4\"][\"model_dir\"]\n",
    "    else:\n",
    "        print(\"Skipping training (steps 1-4), using existing model\")\n",
    "        if not (Path(model_dir) / \"best_model.pth\").exists():\n",
    "            print(f\"  ERROR: No model found at {model_dir}/best_model.pth\")\n",
    "            return outputs\n",
    "        \n",
    "        stats_file = Path(OUTPUT_DIR) / \"landsat_stats.json\"\n",
    "        if stats_file.exists():\n",
    "            with open(stats_file, \"r\") as f:\n",
    "                landsat_stats = json.load(f)\n",
    "    \n",
    "    outputs[\"step5\"] = step5_detect_overview(\n",
    "        model_dir=str(model_dir),\n",
    "        center_lat=center_lat,\n",
    "        center_lon=center_lon,\n",
    "        radius_km=radius_km,\n",
    "        overview_max_dimension=overview_max_dimension,\n",
    "        classification_distance_km=classification_distance_km,\n",
    "        landsat_stats=landsat_stats\n",
    "    )\n",
    "    \n",
    "    outputs[\"step6\"] = step6_validate(\n",
    "        model_dir=str(model_dir),\n",
    "        validation_csv=validation_csv or VALIDATION_CSV,\n",
    "        known_coordinates=known_coordinates,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PIPELINE COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if outputs.get(\"step5\") and not outputs[\"step5\"].get(\"skipped\"):\n",
    "        n_mining = outputs[\"step5\"].get(\"n_confirmed_mining\", 0)\n",
    "        print(f\"  Detections: {n_mining} mining zones detected\")\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"Illegal Mining Detection Pipeline\")\n",
    "    parser.add_argument(\"--skip-training\", action=\"store_true\", help=\"Skip training, use existing model\")\n",
    "    parser.add_argument(\"--skip-data\", action=\"store_true\", help=\"Skip data collection\")\n",
    "    parser.add_argument(\"--center\", type=str, help=\"Center coordinates: LAT,LON (e.g., -14.2,-49.4)\")\n",
    "    parser.add_argument(\"--radius\", type=float, default=OVERVIEW_RADIUS_KM, help=\"Radius from center in km (default: 10)\")\n",
    "    parser.add_argument(\"--overview-size\", type=int, default=2048, help=\"Max dimension for overview (px)\")\n",
    "    parser.add_argument(\"--classification-distance\", type=float, default=2.5, help=\"Distance for classification crops (km)\")\n",
    "    parser.add_argument(\"--validate\", type=str, help=\"Path to validation CSV\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    center_lat, center_lon = None, None\n",
    "    if args.center:\n",
    "        parts = args.center.split(\",\")\n",
    "        if len(parts) == 2:\n",
    "            center_lat = float(parts[0])\n",
    "            center_lon = float(parts[1])\n",
    "        else:\n",
    "            print(\"Error: --center must be LAT,LON (e.g., -14.2,-49.4)\")\n",
    "            exit(1)\n",
    "    \n",
    "\n",
    "    # python3 main.py --skip-training --center=\"-14.2,-49.4\" --radius 10 --overview-size 2048\n",
    "    main(\n",
    "        skip_training=args.skip_training,\n",
    "        skip_data_collection=args.skip_data,\n",
    "        center_lat=center_lat,\n",
    "        center_lon=center_lon,\n",
    "        radius_km=args.radius,\n",
    "        overview_max_dimension=args.overview_size,\n",
    "        classification_distance_km=args.classification_distance,\n",
    "        validation_csv=args.validate\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
